#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# SPDX-License-Identifier: MIT
# Copyright (c) 2025 Jon Paul Lundquist
"""
Created on Mon Sep 29 13:20:42 2025

Consider this a rough draft...
Mostly generated by ChatGPT-5 edited to actually be able to use the L-test 
(it messed up pretty good at that) and corrected the "Empirical size under H0"

Output of example looks good:
    
Power (t3_vs_normal): {'alpha': 0.05, 'n': 200, 'trials': 3000, 
                       'alt_kind': 't3_vs_normal', 'mu_sd': 2.0, 'logsig_sd': 0.35, 
                       'power': 0.7626666666666667, 
                       'power_CI95': (0.7471125951127453, 0.7775489163568836), 
                       'beta': 0.23733333333333329, 
                       'beta_CI95': (0.2224510836431164, 0.2528874048872547)}
Worst-case at n=200: {'alt': 'skew2_vs_normal', 'power': 0.6968, 'beta': 0.3032}
Empirical size under H0: {'alpha': 0.05, 'n': 200, 'trials': 1000, 'alt_kind': 'null', 
                          'mu_sd': 2.0, 'logsig_sd': 0.35, 'power': 0.045, 
                          'power_CI95': (0.03379951228563171, 0.059682837936223455), 
                          'beta': 0.955, 'beta_CI95': (0.9403171620637766, 0.9662004877143683)}
    
@author: Jon Paul Lundquist
"""

# ======================= L-test Power / Type II Harness =======================
# Requirements: numpy (SciPy optional for exact Clopper–Pearson CI).
# Assumes ltest(x, y) returns:
#   - a p-value float

import math
import numpy as np
from ltest import ltest

# ---------- OPTIONAL: exact binomial CI if SciPy is installed ----------
try:
    from scipy.stats import beta as _beta_dist
    _HAVE_SCIPY = True
except Exception:
    _HAVE_SCIPY = False

# --------------------------- Binomial CI for power ---------------------------
def binom_ci(k, n, alpha=0.05, method="wilson"):
    if n == 0:
        return (0.0, 1.0)
    p = k / n
    if method == "clopper" and _HAVE_SCIPY:
        a = alpha / 2.0
        lo = 0.0 if k == 0 else _beta_dist.ppf(a, k, n - k + 1)
        hi = 1.0 if k == n else _beta_dist.ppf(1 - a, k + 1, n - k)
        return (lo, hi)
    # Wilson (good approx; no SciPy needed)
    z = 1.959963984540054
    denom = 1.0 + (z*z)/n
    center = (p + (z*z)/(2*n)) / denom
    half = (z/denom) * math.sqrt(p*(1-p)/n + (z*z)/(4*n*n))
    return (max(0.0, center - half), min(1.0, center + half))

# ---------------------------- Parameterized families ----------------------------
def sample_family(rng, n, family, mu, sigma):
    """
    Draw n samples directly parameterized by location mu and (std-like) scale sigma.
    No post-processing. Variance is part of the shape.
    Families:
      - "normal":      N(mu, sigma^2)
      - "laplace":     Laplace(mu, b) with b = sigma/sqrt(2) so SD ≈ sigma
      - "t3":          Student t (df=3) with scale s so SD ≈ sigma
      - "mix2":        Symmetric mixture 0.5*N(mu-δσ, σ^2) + 0.5*N(mu+δσ, σ^2)
      - "skew2":       Two-piece normal around mu with left/right scales (k controls skew)
    """
    if family == "normal":
        return rng.normal(loc=mu, scale=sigma, size=n)

    elif family == "laplace":
        b = sigma / math.sqrt(2.0)  # Var=2b^2 => SD≈sigma
        return rng.laplace(loc=mu, scale=b, size=n)

    elif family == "t3":
        df = 3.0
        # For t_df with scale s: Var = s^2 * df/(df-2). Choose s so SD ≈ sigma.
        s = sigma * math.sqrt((df - 2.0) / df)
        return mu + s * rng.standard_t(df, size=n)

    elif family == "mix2":
        δ = 1.25
        # Component means spaced by ±δσ; component SD = σ (so mixture Var ≈ σ^2(1+δ^2))
        signs = rng.choice([-1.0, 1.0], size=n)
        centers = mu + signs * (δ * sigma)
        return rng.normal(loc=centers, scale=sigma, size=n)

    elif family == "skew2":
        # Two-piece normal: left side σL = σ * c, right side σR = σ * k * c.
        # Choose c so central tendency/scale follow σ while enabling skew via k.
        k = 1.8  # >1 -> heavier right tail; set k<1 for left skew
        p_right = k / (1.0 + k)
        c = math.sqrt(2.0 / (1.0 + k*k))
        σL = sigma * c
        σR = sigma * c * k
        u = rng.random(n)
        left_mask = u >= p_right
        x = np.empty(n)
        x[~left_mask] = rng.normal(loc=mu, scale=σR, size=(~left_mask).sum())
        x[left_mask]  = rng.normal(loc=mu, scale=σL, size=(left_mask).sum())
        return x

    else:
        raise ValueError(f"Unknown family '{family}'")

def _shared_sigma(rng, logsig_sd):
    # one lognormal draw per trial, used for BOTH samples under H0
    return math.exp(rng.normal(0.0, logsig_sd))

# -------------------------- Trial sampler with priors --------------------------
def draw_pair(rng, n, alt_kind, mu_sd=2.0, logsig_sd=0.35, null_family="normal"):
    """
    Each trial:
      μ_x, μ_y ~ N(0, mu_sd^2)
      For alternatives: log σ_x, log σ_y ~ N(0, logsig_sd^2) (independent)
      For H0 ("null"):  use a SHARED σ for x and y (same shape!)
    """
    mu_x = rng.normal(0.0, mu_sd)
    mu_y = rng.normal(0.0, mu_sd)

    if alt_kind == "null":
        # H0: same shape. Same family AND same σ within the trial.
        sigma = _shared_sigma(rng, logsig_sd)
        fam_x = fam_y = null_family
        x = sample_family(rng, n, fam_x, mu=mu_x, sigma=sigma)
        y = sample_family(rng, n, fam_y, mu=mu_y, sigma=sigma)
        return x, y

    # otherwise (alternatives): allow σ_x ≠ σ_y
    sigma_x = math.exp(rng.normal(0.0, logsig_sd))
    sigma_y = math.exp(rng.normal(0.0, logsig_sd))

    if alt_kind == "laplace_vs_normal":
        fam_x, fam_y = "normal", "laplace"
    elif alt_kind == "t3_vs_normal":
        fam_x, fam_y = "normal", "t3"
    elif alt_kind == "mix2_vs_normal":
        fam_x, fam_y = "normal", "mix2"
    elif alt_kind == "skew2_vs_normal":
        fam_x, fam_y = "normal", "skew2"
    elif alt_kind == "variance_mismatch":
        fam_x, fam_y = "normal", "normal"  # same family, different σ
    else:
        raise ValueError(f"Unknown alt_kind '{alt_kind}'")

    x = sample_family(rng, n, fam_x, mu=mu_x, sigma=sigma_x)
    y = sample_family(rng, n, fam_y, mu=mu_y, sigma=sigma_y)
    return x, y

# -------------------------- Power / Type II estimator --------------------------
def estimate_type2(n=200, B=2000, alpha=0.05, alt_kind="laplace_vs_normal",
                   seed=0, mu_sd=2.0, logsig_sd=0.35, null_family="normal",
                   ci_method="wilson"):
    rng = np.random.default_rng(seed)
    rejections = 0
    for _ in range(B):
        x, y = draw_pair(rng, n, alt_kind, mu_sd=mu_sd, logsig_sd=logsig_sd, null_family=null_family)
        p, *_ = ltest(x, y)
        if p < alpha:
            rejections += 1
    power = rejections / B
    beta = 1.0 - power
    lo_pow, hi_pow = binom_ci(rejections, B, alpha=0.05, method=ci_method)
    lo_beta, hi_beta = 1.0 - hi_pow, 1.0 - lo_pow
    return {
        "alpha": alpha, "n": n, "trials": B, "alt_kind": alt_kind,
        "mu_sd": mu_sd, "logsig_sd": logsig_sd,
        "power": power, "power_CI95": (lo_pow, hi_pow),
        "beta": beta, "beta_CI95": (lo_beta, hi_beta),
    }

# ------------------------------- Convenience sweeps -------------------------------
def sweep_alternatives(alts=("laplace_vs_normal","t3_vs_normal","mix2_vs_normal",
                             "skew2_vs_normal","variance_mismatch"),
                       n=200, B=2000, alpha=0.05, seed=0,
                       mu_sd=2.0, logsig_sd=0.35, null_family="normal",
                       ci_method="wilson"):
    results = []
    for i, alt in enumerate(alts):
        results.append(
            estimate_type2(n=n, B=B, alpha=alpha, alt_kind=alt,
                           seed=seed+i, mu_sd=mu_sd, logsig_sd=logsig_sd,
                           null_family=null_family, ci_method=ci_method)
        )
    return results

def sweep_sample_sizes(ns=(50,100,200,400,800),
                       alt_kind="laplace_vs_normal", B=2000, alpha=0.05, seed=0,
                       mu_sd=2.0, logsig_sd=0.35, null_family="normal",
                       ci_method="wilson"):
    results = []
    for i, n in enumerate(ns):
        results.append(
            estimate_type2(n=n, B=B, alpha=alpha, alt_kind=alt_kind,
                           seed=seed+i, mu_sd=mu_sd, logsig_sd=logsig_sd,
                           null_family=null_family, ci_method=ci_method)
        )
    return results

# ------------------------------- Type I (size) -------------------------------
def estimate_type1(n=200, B=2000, alpha=0.05, seed=0,
                   mu_sd=2.0, logsig_sd=0.35, null_family="normal",
                   ci_method="wilson"):
    return estimate_type2(n=n, B=B, alpha=alpha, alt_kind="null",
                          seed=seed, mu_sd=mu_sd, logsig_sd=logsig_sd,
                          null_family=null_family, ci_method=ci_method)

# ----------------------------------- Demo -----------------------------------
if __name__ == "__main__":

    # Power under a heavy-tail alt with randomized μ, σ every trial
    res = estimate_type2(n=200, B=3000, alpha=0.05,
                         alt_kind="t3_vs_normal", mu_sd=2.0, logsig_sd=0.35, seed=0)
    print("Power (t3_vs_normal):", res)

    # Worst-case over several shapes at a fixed n
    worst = min(sweep_alternatives(n=200, B=3000, alpha=0.05, seed=10),
                key=lambda r: r["power"])
    print("Worst-case at n=200:", {"alt": worst["alt_kind"], "power": worst["power"], "beta": worst["beta"]})

    # Type I (size) check with the SAME μ, σ priors
    size = estimate_type1(n=200, B=1000, alpha=0.05, seed=123)
    print("Empirical size under H0:", size)
# ========================= End L-test Power Harness ===========================