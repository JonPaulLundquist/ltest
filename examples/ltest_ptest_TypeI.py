#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# SPDX-License-Identifier: MIT
# Copyright (c) 2025 Jon Paul Lundquist
"""
Created on Fri Sep 26 13:17:46 2025

Consider this a rough draft...
Half generated by ChatGPT-5 to test different p-value calculations for the L-test.
Rejection percentage should be close to alpha which it is for version A
Also, used to test accuracy of the L-test shift value for different minimizers

Some example runs with not too many trials and the 500 bootstrap, 5% p-val error defaults:
    
res = compare_methods(nx=200, ny=200, dist="gauss",
                       shift=0.0, n_trials=200, B=500, alpha=0.05)
print_results(res, alpha=0.05)
alpha = 0.05
shift	TypeI/power	time(s)
0.000		0.015			363.442

Gets better with more trials:
res = compare_methods(nx=200, ny=200, dist="gauss",
                       shift=10.0, n_trials=500, B=500, alpha=0.05)
print_results(res, alpha=0.05)
alpha = 0.05
shift	TypeI/power	time(s)
10.000		0.054			864.099

res = compare_methods(nx=200, ny=200, dist="gauss",
                       shift=10.0, n_trials=200, B=500, alpha=0.05)
print_results(res, alpha=0.01)
alpha = 0.05
shift	TypeI/power	time(s)
10.000		0.040			344.246

res = compare_methods(nx=200, ny=200, dist="gauss",
                       shift=0.0, n_trials=200, B=500, alpha=0.01)
print_results(res, alpha=0.01)
alpha = 0.01
shift	TypeI/power	time(s)
0.000		0.005			340.377

res = compare_method(nx=200, ny=200, dist="gauss",
                       shift=10.0, n_trials=200, B=500, alpha=0.01)
print_results(res, alpha=0.01)
alpha = 0.01
shift	TypeI/power	time(s)
10.000		0.015			341.343

res = compare_methods(nx=200, ny=200, dist="laplace",
                       shift=5.0, n_trials=500, B=500, alpha=0.01)
print_results(res, alpha=0.01)
alpha = 0.01
shift	TypeI/power	time(s)
5.000		0.010			1038.693

@author: Jon Paul Lundquist
"""

import time
import numpy as np
from ltest import l_stats, l_pval #, l_pval_b, l_pval_c
#from os import cpu_count

# --- Data generators (Var≈1 for comparability) ---
def gen_samples(nx, ny, dist="gauss", shift=0.0, rng=None):
    if rng is None: rng = np.random.default_rng()
    if dist == "gauss":
        x = rng.normal(0.0, 1.0, size=nx)
        y = rng.normal(shift, 1.0, size=ny)
    elif dist == "laplace":
        # scale so Var=1
        b = 1/np.sqrt(2)
        x = rng.laplace(0.0, b, size=nx)
        y = rng.laplace(shift, b, size=ny)
    elif dist == "t3":
        # Student-t df=3 scaled to Var≈1
        s = np.sqrt(3/(3-2))
        x = rng.standard_t(3, size=nx)/s
        y = rng.standard_t(3, size=ny)/s + shift
    else:
        raise ValueError("dist must be 'gauss', 'laplace', or 't3'")
    return x, y

# --- Simulation harness using your l_pval_a/b/c ---
def compare_methods(nx=200, ny=200, dist="gauss", shift=0.0, n_trials=200, 
                    B=1000, alpha=0.05, tol_p=0.01):
    
    rng = np.random.default_rng()
    
    k, N = nx*ny, nx + ny
    ix = np.arange(1, nx+1)
    iy = np.arange(1, ny+1)

    rejA = 0 
    #rejB = rejC = 0
    tA = 0
    #tB = tC = 0.0
    shifts = np.zeros(n_trials)
    mean_shift = np.zeros(n_trials)
    shift_std = np.zeros(n_trials)
    shiftA = np.zeros(n_trials)
    #shiftB = np.zeros(n_trials)
    #shiftC = np.zeros(n_trials)
    shiftA_err = np.zeros(n_trials)
    #shiftB_err = np.zeros(n_trials)
    #shiftC_err = np.zeros(n_trials)
    for i in range(n_trials):
        #print(f"Trial # {i}")
        # Generate a trial dataset
        x, y = gen_samples(nx, ny, dist, shift, rng)
        
        xa = np.sort(x)
        ya = np.sort(y)
        
        # Compute observed stats once: l_shift_obs, u_min (threshold) for the right tail
        l_shift_obs, u_min = l_stats(xa, ya, nx, ny, ix, iy, k, N, brute=False, stat='cvm')
        
        shifts[i] = l_shift_obs
        
        mean_shift[i] = np.mean(xa)-np.mean(ya)
        #print("Method A")
        # --- Method A ---
        t0 = time.perf_counter()
        pA, _, sA, sA_err = l_pval(xa, ya, nx, ny, ix, iy, k, N, u_min, B, False, 
                                   tol_p=tol_p, stat='cvm')
        tA += time.perf_counter() - t0
        rejA += (pA <= alpha)
        shiftA[i] = sA
        shiftA_err[i] = sA_err
        
        #print("Method B")
        # --- Method B ---
        # t0 = time.perf_counter()
        # pB, _, sB, sB_err = l_pval_b(xa, ya, nx, ny, ix, iy, k, N, u_min, l_shift_obs, B, tol_rel=tol_rel)
        # tB += time.perf_counter() - t0
        # rejB += (pB <= alpha)
        # shiftB[i] = sB
        # shiftB_err[i] = sB_err
        
        #print("Method C")
        # --- Method C ---
        # t0 = time.perf_counter()
        # pC, _, sC, sC_err = l_pval_c(xa, ya, nx, ny, ix, iy, k, N, u_min, l_shift_obs, B, tol_rel=tol_rel)
        # tC += time.perf_counter() - t0
        # rejC += (pC <= alpha)
        # shiftC[i] = sC
        # shiftC_err[i] = sC_err
        
        typeI_A = rejA / n_trials
        #typeI_B = rejB / n_trials
        #typeI_C = rejC / n_trials

    shift_mean = shifts.mean()
    shift_std = shifts.std()
    shiftA_mean = shiftA.mean()
    shiftA_std = shiftA.std(ddof=1)
    shiftA_err_mean = shiftA_err.mean()
    shiftA_err_std = shiftA_err.std(ddof=1)
    #results = (shift, typeI_A, typeI_B, typeI_C,
    #           tA, tB, tC, shift_mean, shift_std, shiftA_mean, shiftA_std, shiftA_err_mean, shiftA_err_std, np.std(mean_shift))
    
    results = (shift, typeI_A, tA, shift_mean, shift_std, shiftA_mean, shiftA_std, 
               shiftA_err_mean, shiftA_err_std, np.std(mean_shift))

    return results


# --- Pretty print ---
def print_results(res, alpha=0.05):
    print(f"alpha = {alpha}")
    #print("shift\tA(typeI/power)\tB(typeI/power)\tC(typeI/power)\tA_time(s)\tB_time(s)\tC_time(s)")
    #print(f"{res[0]:.3f}\t\t{res[1]:.3f}\t\t\t{res[2]:.3f}\t\t\t{res[3]:.3f}\t\t{res[4]:.2f}\t\t{res[5]:.2f}\t\t{res[6]:.2f}")
    #print(f"Mean Shift Error:{np.std(res[13])}")
    
    print("shift\tTypeI/power\ttime(s)")
    print(f"{res[0]:.3f}\t\t{res[1]:.3f}\t\t\t{res[2]:.3f}")
    print(f"Mean Shift Error:{np.std(res[9])}")
 
# ----------------------------------- Demo -----------------------------------
if __name__ == "__main__":
    
    res = compare_methods(nx=200, ny=200, dist="gauss",
                          shift=0.0, n_trials=200, B=500, alpha=0.05)
    print_results(res, alpha=0.05)